{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8  ('SWQU_ENV': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "5f8af38b413bd962c91e477d0fb0938995ececd446d49914465dc9a00018a1f6"
   }
  },
  "interpreter": {
   "hash": "bc4c4f08932c4311f142a5dd61637fc57491ca6ec84d615f43d2f1826eb4d8d9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGES\n",
    "%reset -f\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from scipy import stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "import properscoring as ps\n",
    "from IPython.display import clear_output\n",
    "# from sklearn.calibration import calibration_curve\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.isotonic import IsotonicRegression\n",
    "# from statsmodels.distributions.empirical_distribution import ECDF\n",
    "# import pandas as pd"
   ]
  },
  {
   "source": [
    "# FUNCTIONS\n",
    "def gp_predict(x_data,y_data):\n",
    "    # Mesh the input space for multiple evaluations\n",
    "    x_data = np.atleast_2d(x_data).T\n",
    "    # Straighten out the observations array? Why did I have to do this?\n",
    "    y_data = y_data.ravel()\n",
    "    # Inisiate a Gaussian Process model\n",
    "    # kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2)) # An obscenely overdetermined kernal if you want to hit every single point. But we don't.\n",
    "    gp = GaussianProcessRegressor(n_restarts_optimizer=9)\n",
    "    # Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "    gp.fit(x_data,y_data)\n",
    "    # Make the prediction on the meshed x-axis (ask for MSE as well)\n",
    "    y_pred, sigma = gp.predict(x_data, return_std=True)\n",
    "\n",
    "    skew_predict_from_data = 0\n",
    "    # skew_predict_from_data = 4\n",
    "    y_pred+=skew_predict_from_data\n",
    "    return y_pred, sigma\n",
    "\n",
    "def plot_data_and_predictions(x_data,y_data,xf,yf,y_pred,title,legend_on):\n",
    "    input_data = plt.scatter(x_data,y_data, color = 'k', facecolor = 'none' , label = 'Data')\n",
    "    gp_prediction = plt.plot(x_data,y_pred,'b-' ,label = 'GP Prediction')\n",
    "    true_function = plt.plot(xf,yf,'r-',label = 'True Function')\n",
    "    if legend_on:\n",
    "        plt.legend()\n",
    "    plt.title(title, fontweight='bold')\n",
    "def get_pdf(data):\n",
    "    # Sample from a normal distribution using numpy's random number generator\n",
    "    samples = data\n",
    "    bin_min = np.min(data)\n",
    "    bin_max = np.max(deta)\n",
    "    n_bins = 20\n",
    "    # Compute a histogram of the sample\n",
    "    bins = np.linspace(bin_min,bin_max,n_bins)\n",
    "    histogram, bins = np.histogram(samples, bins=bins, density=True)\n",
    "    bin_centers = 0.5*(bins[1:] + bins[:-1])\n",
    "    # Compute the PDF on the bin centers from scipy distribution object\n",
    "    pdf = st.norm.pdf(bin_centers)\n",
    "    return pdf\n",
    "\n",
    "def reliability_curves(y_true,y_pred,num_bins):\n",
    "    for ii in range(y_true.size):\n",
    "        yt = y_true[ii]\n",
    "        yp = y_pred[ii]\n",
    "        \n",
    "        prob_true, prob_pred = calibration_curve(yt, yp,normalize=true, n_bins=num_bins)\n",
    "        plt.plot(prob_pred,prob_true,'o-')\n",
    "    perfect_calibration = linspace(0,1,100)\n",
    "    plt.plot(perfect_calibration,perfect_calibration,'k-')\n",
    "\n",
    "def make_gaus_noisy_data(mu_f,sig_f):\n",
    "    n_samples = mu_f.size\n",
    "    y_data = np.random.normal(mu_f,sig_f,n_samples)\n",
    "    return y_data\n",
    "\n",
    "\n",
    "# RS IS BEING CALCULATED INCORRECTLY AS OF 6/17. THE FOLLOWING TWO FUNCTIONS NEED TO BE REWORKED TO CORRECT THIS.\n",
    "# \n",
    "# standard_error()\n",
    "# get_RS()\n",
    "def standard_error(y_data,y_pred):\n",
    "    epsilon = y_data - y_pred\n",
    "    sigma = np.std(np.vstack((y_data,y_pred)),axis=0) \n",
    "    # THIS IS WRONG AS OF 6/17 !!!! CORRECT ASAP\n",
    "\n",
    "    eta = epsilon / (np.sqrt(2)*sigma)\n",
    "    return eta, sigma, epsilon\n",
    "def get_RS(eta):\n",
    "    phi = 0.5*(sc.special.erf(eta) + 1)\n",
    "    C = np.empty((eta.size,))*0\n",
    "    for ii in range(eta.size):\n",
    "        C[ii] = np.mean(np.heaviside(eta[ii] - eta,eta*0))\n",
    "    RS = np.sum(np.abs(phi - C)**2)\n",
    "    N = eta.size\n",
    "    ind = np.linspace(1,N)\n",
    "\n",
    "    # Please note, at sufficiently high sample size (N) the RS_min will asymptote at RS_min = 0.4 \n",
    "\n",
    "    RS_min = (1/(np.sqrt(np.pi)*N)) * np.sum( np.exp(-1*sc.special.erfinv( ((2*ind-1)/N) - 1)**2) - ((1/2)*np.sqrt(2/np.pi)) )\n",
    "    return RS, RS_min\n",
    "# \n",
    "# \n",
    "# \n",
    "\n",
    "def get_crps(y_data,y_pred):\n",
    "    \n",
    "    # Camporeale's analytic definition of CRPS (below) does not seem to EXACTLY recreate the CRPS in sklearn or properscoring but it is EXTREMELY close, on average within 3-5%.\n",
    "    # I'm using his definition of CRPS in the paper for the sake of consistency.\n",
    "    eta, sigma, epsilon = standard_error(y_data,y_pred)\n",
    "    crps = sigma*( epsilon/sigma * sc.special.erf( epsilon/(np.sqrt(2)*sigma) ) + np.sqrt(2/np.pi)*np.exp( (-epsilon**2)/(2*sigma**2) ) - ( 1/np.sqrt(np.pi) ) )\n",
    "    # properscoring's crps output is near identical to Camporeale's definition. So that's good.\n",
    "    # crps = ps.crps_ensemble(y_data,y_pred)\n",
    "    crps_min = (np.sqrt(np.log(4))/2)*np.mean(epsilon)\n",
    "    return crps, crps_min\n",
    "\n",
    "def get_BETA(RS_min,CRPS_min):\n",
    "    Beta = RS_min/(CRPS_min+RS_min)\n",
    "    return Beta\n",
    "\n",
    "def get_ACCRUE(Beta,crps,RS):\n",
    "    CRPS_mean = np.mean(crps)\n",
    "    ACCRUE = Beta * CRPS_mean + (1-Beta) * RS\n",
    "    return ACCRUE\n",
    "\n",
    "def make_data_model_ensemble(xf,mu_f,sig_f,N_models):\n",
    "    y_pred = np.empty((N_models,mu_f.size))\n",
    "    y_true = np.empty((N_models,mu_f.size))\n",
    "    pred_sigma = np.empty((N_models,mu_f.size))\n",
    "    for ii in range(N_models):\n",
    "        yt = make_gaus_noisy_data(mu_f,sig_f)\n",
    "        yp,sigma = gp_predict(xf,yt)\n",
    "        y_pred[ii] = yp\n",
    "        y_true[ii] = yt\n",
    "        pred_sigma[ii] = sigma\n",
    "    y_pred = np.squeeze(y_pred)\n",
    "    y_true = np.squeeze(y_true)\n",
    "    return y_pred, pred_sigma, y_true\n",
    "\n",
    "def force_prediction_errors(y_pred,errors):\n",
    "    N = y_predict_ensemble.shape\n",
    "    N = N[0]\n",
    "    for ii in range(N):\n",
    "        y_pred[ii] = y_pred[ii] + errors[ii]\n",
    "    return y_pred\n",
    "    \n",
    "def camporeale_toy_problems(name,N_points):\n",
    "    if name == 'W':\n",
    "        # W toy problem\n",
    "        x_min = 0\n",
    "        x_max = np.pi\n",
    "        xf = np.linspace(x_min,x_max,int(N_points))\n",
    "        yf = np.sin(2.5*xf)*np.sin(1.5*xf)\n",
    "        mu_f = yf\n",
    "        sig_f = 0.01 + 0.25*(1-np.sin(2.5*xf))**2\n",
    "        title = 'W'\n",
    "        y_data = make_gaus_noisy_data(mu_f,sig_f)\n",
    "        y_pred,sigma = gp_predict(xf,y_data)\n",
    "    elif name == 'Y':\n",
    "        # Y toy problem\n",
    "        x_min = 0\n",
    "        x_max = 1; \n",
    "        xf = np.linspace(x_min,x_max,int(N_points))\n",
    "        yf = 2*(np.exp(-30*(xf-0.25)**2) + np.sin(np.pi*xf**2)) - 2\n",
    "        mu_f = yf\n",
    "        sig_f = np.exp(np.sin(2*np.pi*xf))/3\n",
    "        title = 'Y'\n",
    "        y_data = make_gaus_noisy_data(mu_f,sig_f)\n",
    "        y_pred,sigma = gp_predict(xf,y_data)\n",
    "    elif name == 'G':\n",
    "        # G toy problem\n",
    "        x_min = 0\n",
    "        x_max = 1\n",
    "        xf = np.linspace(x_min,x_max,int(N_points))\n",
    "        yf = 2*np.sin(2*np.pi*xf)\n",
    "        mu_f = yf\n",
    "        sig_f = 0.5*xf+0.5\n",
    "        title = 'G'\n",
    "        y_data = make_gaus_noisy_data(mu_f,sig_f)\n",
    "        y_pred,sigma = gp_predict(xf,y_data)\n",
    "    elif name == '5D':\n",
    "        # 5D toy problem\n",
    "        x_min = 0\n",
    "        x_max = 1\n",
    "        N_points = 7 # N_points is enforced at 7 points as a more complex vector will stress a standard machine when meshed over 5 dimensions, just for this specific toy problem.\n",
    "        # Make a 5-dimensional matrix where the columns comprise every possible permutation of the variables within the 5-D space\n",
    "        xf = np.linspace(x_min,x_max,N_points); x1,x2,x3,x4,x5 = np.meshgrid(xf,xf,xf,xf,xf);xf_meshed = np.vstack((x1.ravel(),x2.ravel(),x3.ravel(),x4.ravel(),x5.ravel()));del x1,x2,x3,x4,x5\n",
    "        xf = xf_meshed[4,:] # got to choose a 1-D axis for this toy problem as Camporeale reduces this 5-D problem to 1-D analysis.\n",
    "        sig_f = 0.45*(np.cos(np.pi + np.sum(xf_meshed*5,axis=0)) + 1.2)\n",
    "        yf = sig_f*0\n",
    "        mu_f = yf\n",
    "        title = '5D Toy Problem'\n",
    "        y_data = 0 #make_gaus_noisy_data(mu_f,sig_f)\n",
    "        y_pred = 0\n",
    "        sigma =  0 #gp_predict(xf,y_data)\n",
    "\n",
    "    title = title\n",
    "    x_data = xf\n",
    "    y_true = yf\n",
    "    y_data = y_data\n",
    "    y_pred = y_pred\n",
    "    sig_true = sig_f\n",
    "    sig_pred = sigma\n",
    "    return x_data, y_data, y_true, y_pred, sig_true, sig_pred, title\n",
    "\n",
    "\n",
    "# end functions"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del y_data, y_pred, eta, sigma, epsilon, CRPS, CRPS_min, RS, RS_min, Beta, ACCRUE, RS_vec, CRPS_vec\n",
    "# Toy_Problems = 'W'\n",
    "# Toy_Problems = 'G'\n",
    "# Toy_Problems = 'Y'\n",
    "Toy_Problems = ['W','G','Y']\n",
    "N_models = 500\n",
    "N_points = 100 # per model\n",
    "N_problems = len(Toy_Problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(N_problems,2)\n",
    "fig.set_size_inches([15,15])\n",
    "row=-1\n",
    "for jj in range(N_problems):\n",
    "    row+=1\n",
    "    curr_Toy_Problem = Toy_Problems[jj]\n",
    "    x_data, y_data, y_true, y_pred, sig_true, sig_pred, title = camporeale_toy_problems(curr_Toy_Problem,N_points)\n",
    "    y_predict_ensemble, ensemble_sigma, y_data_ensemble = make_data_model_ensemble(x_data,y_true,sig_true,N_models)\n",
    "    clear_output(wait=True)\n",
    "    ACCRUE = np.empty((N_models,))*0; RS_vec = np.empty((N_models,))*0; CRPS_vec = np.empty((N_models,))*0\n",
    "\n",
    "\n",
    "    y_predict_ensemble = force_prediction_errors(y_predict_ensemble,ensemble_sigma)\n",
    "\n",
    "\n",
    "    for ii in range(N_models):\n",
    "        y_data = y_data_ensemble[ii]\n",
    "        y_pred = y_predict_ensemble[ii]\n",
    "        eta, sigma, epsilon = standard_error(y_data,y_pred)\n",
    "        crps, CRPS_min = get_crps(y_data,y_pred)\n",
    "        RS, RS_min = get_RS(eta)\n",
    "        Beta = get_BETA(RS_min,CRPS_min)\n",
    "        ACCRUE[ii] = get_ACCRUE(Beta,crps,RS)\n",
    "        RS_vec[ii] = RS\n",
    "        CRPS_vec[ii] = np.mean(crps)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "\n",
    "    # sort_ind = np.argsort(ACCRUE)\n",
    "    # ACCRUE = ACCRUE[sort_ind]\n",
    "    # CRPS_vec = CRPS_vec[sort_ind]\n",
    "    # RS_vec = RS_vec[sort_ind]\n",
    "\n",
    "    if N_problems>1:\n",
    "        plt.axes(axs[row,0])\n",
    "    else:\n",
    "        plt.axes(axs[0])\n",
    "\n",
    "    for ii in range(N_models):\n",
    "        plot_data_and_predictions(x_data,y_data_ensemble[ii],x_data,y_true,y_predict_ensemble[ii],'model/data ensemble: ' + title + '  ' + str(N_models) + ' model(s)',0)\n",
    "\n",
    "    if N_problems>1:\n",
    "        plt.axes(axs[row,1])\n",
    "    else:\n",
    "        plt.axes(axs[1])\n",
    "\n",
    "    scatterpl = plt.scatter(CRPS_vec,RS_vec,c = ACCRUE)\n",
    "    scatterpl.set_clim(np.min(ACCRUE),np.max(ACCRUE))\n",
    "    plt.xlabel('Continuous Ranked Probability Score (CRPS)', fontweight='bold')\n",
    "    plt.ylabel('Reliability Score (RS)', fontweight='bold')\n",
    "    plt.title(title + ' toy problem - Model Ensemble - ' + str(N_models) + ' model(s)', fontweight='bold')\n",
    "    cbr = plt.colorbar(scatterpl)\n",
    "    cbr.set_label(label='ACCRUE Score',fontweight='bold')\n",
    "\n",
    "\n",
    "    # Some notes:\n",
    "    #\n",
    "    # In the case of these toy problems, which are all gaussian distributed random data (aka symmetricly distributed statistically), the use of CRPS in the ACCRUE scalar seems to effectively discriminate better definitions of CRPS from other models in the ensembles shown.\n",
    "\n",
    "    # It appears the same cannot be said of the RS score, however. Atleast with the caxis we use here, there shows no capacity to discriminate better RS scores using ACCRUE. In example, the ACCRUE values for a given CRPS (x-axis) seem to be far too similar across all RS scores values (y-axis) at that given CRPS.\n",
    "    # CAUSES:\n",
    "    # (1) Again, the MOST LIKELY cause is I'm just doing something wrong (probably with RS). I'm pretty new to this.\n",
    "    # \n",
    "    # (2) The inability to discriminate RS using ACCRUE may just be a consequence of the symmetry of the observations (black dots) about the true function (red line). \n",
    "    #       -I could test this by applying a static shift to the predictions.\n",
    "    #       ---If the RS score then begins to show a bias after doing so (using ACCRUE) this will increase the spread of the predictions around the true function shich could indicate RS as it is defined ay NOT work for gaussian distributed data or at the very least it needs to be tunes (with Beta) to be more sensitive.\n",
    "    #       ----If that continues to be a developing argument then a better definition for RS is likely required.\n",
    "    #       ----To that end, if an ideal realibility diagram is a perfectly straight line (CDF==PDF) then a scalar quantity that would more intuitively and more exactly reflect the divergence of the realibility from that straight line may be the way to go, such as R^2 or chi-squared.\n",
    "    #       ---If the RS score behavior persists with no change then\n",
    "    #           ---....?\n",
    "    # \n",
    "    # \n",
    "    #\n",
    "    # PRIORITY-->          ---After testing, it seems to be the FIRST scenario. I fucked something up. Only by forcing more variability into the ensemble of predictions can I get the RS score to dramatically discriminate better RS and CRPS values using ACCRUE. When I don't do this, it is too subtle to see in this plot which is not the same as saying it is too subtle to use for discrimination numerically. I need to investigate this more by binning RS scores within windows for small ranges of CRPS.\n"
   ]
  }
 ]
}